{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f75db7ec",
      "metadata": {
        "id": "f75db7ec"
      },
      "source": [
        "# Democratic Deliberation with perspectives\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AI-Agents-Prompts-to-Multi-Agent-Sys/Democratic-Dialogue/blob/main/democraticDeliberation.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27a85002",
      "metadata": {
        "id": "27a85002"
      },
      "source": [
        "## About\n",
        "This notebook implements a [LangGraph framework](https://www.langchain.com/langgraph) (LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows) for the purpose of having a special kind of Democratic Deliberation. You can use this notebook to deliberate discussion for any topic and roles of your choice. We'll start with debating next year's tech trend. In each debate round, the six participating debaters will generate three key viewpoints, and then a representative (think about it as a 'speaker') will summarise their thoughts and rank their expressed votes. After viewing the summarisation, as the [human-in-the-loop](https://cloud.google.com/discover/human-in-the-loop), you can decide whether to continue the deliberation or not. By continuing, each debater will be able to see their own history as well as the representative's summarization and might adjust their opinions, potentially leading to the identification of areas of agreement and disagreement.\n",
        "\n",
        "***If you modified and need to reset this notebook, please simply refresh the browser (you will lose any changes you made).***<br>\n",
        "***If you prefer to make changes and keep them, please click [Copy to Drive] in the menu, and work on it there.***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d0093b",
      "metadata": {
        "id": "28d0093b"
      },
      "source": [
        "## Instruction\n",
        "This project only requires a Gemini API Key to run.\n",
        "\n",
        "---\n",
        "## I. Get a Gemini API key\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> *Skip this step if you already have a valid API key created in Google AI Studio.*\n",
        "\n",
        "To use Gemini in this notebook, you'll need to get an API key from Google AI Studio.\n",
        "\n",
        "1. On the left side panel of this notebook, click the `Secrets` ![secrets.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/secrets.png) tab.\n",
        "2. Find and click on Gemini API keys, then select Manage API keys in Google AI Studio.\n",
        "![manage_key_in_google_ai_studio.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/manage_key_in_google_ai_studio.png)\n",
        "This will open a new browser tab directing you to Google AI Studio.\n",
        "3. In the new tab, login to Google AI Studio and click ![create_api_key.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/create_api_key.png) on the top right to create an API key\n",
        "4. Create a new project or select an existing one, then create an API key. Naming can be arbitrary.\n",
        "![creating_api_key.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/creating_api_key.png)\n",
        "5. Click on the API key you just created and copy it to your clipboard.\n",
        "![copy_api_key.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/copy_api_key.png)\n",
        "\n",
        "## II. Add Gemini API Key to secrets\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> *If a Google API key already exists in Secrets, simply enable it and skip this part.*\n",
        "\n",
        "Once you have your API key from Google AI Studio, you'll need to import it into this notebook:\n",
        "\n",
        "1. Navigate to the `Secrets` ![secrets.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/secrets.png) tab on left\n",
        "2. Click on `Add new secret`\n",
        "3. Name the secret `GEMINI_API_KEY` and paste your API key into the value field\n",
        "4. Turn on the secret by toggling the switch next to it\n",
        "![adding_secret.png](https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Quantitative-Eval/master/images/adding_secret.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4759a1e1",
      "metadata": {
        "id": "4759a1e1"
      },
      "source": [
        "---\n",
        "## Runing the notebook\n",
        "\n",
        "\n",
        "Now you' re ready to run the code and interact with the notebook.\n",
        "- Every code cell in this notebook has a **run button** ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAdCAIAAAAyxktbAAAB5UlEQVR4AeyTL4tCQRTFl0XMBpN+Ai1Gi8EoNotlg8E/RYwWQYOCxSgW/wTDFotNjAaL0aKfQJNh24IY9ocDw7zrzHvCImzY4TKce+45h3nvzXv/ftl6f3vZ+o8Wr/YPvJCv+xLn8m8DTr1arVqtVqFQ+LgvAC2kf6iaOqN3u121Wh2NRvv9/nq9KjWAFpIRAkW6dns05+p2u+fz2WVjhACZSwBvieY4nItZYCFD7JJZoqfTqanOZDLJZNJkTCzE5khG84w8rKkADwaDRqMRjUbBohBjEaRqZfR2u1UDsedyudlsViwWBU/rssjo4/GI2lqhUKhUKo3H42w2awpcFk80vwXXy7Q94ng83mw2e72e/gBYMD4qPdGP498wnuhIJBIOh/3jTqcTX7Xdbh8OB6XEglFhc/dEM0gkEuzWut1u8/m8VqttNhtT4LLIaG6xadN4vV6Xy+XFYqEZDVwWGZ3P52OxmLYpwHcbDoeXy0W15o4Yi8loLKMZVCoVdl1cW/1aNamBEGseYIlOp9P1ep1ZYCFD7JJZopHyjJ1Oh4cFW4sRAmTWqSLt0cw4zmQy4VypVIrrBUMBaCEZIYDxKWe08nCufr+/XC4/7wtAC6mm/ntAtDbzU1C6fQY8G/1MltC8MPoHAAD//3a67YoAAAAGSURBVAMAtzELIMv0a+kAAAAASUVORK5CYII=) that appears in the top-left corner when you hover your mouse over the cell. Click on this button to run the cell.\n",
        "- Alternatively, you can select the cell and press `Shift` + `Enter` to run it.\n",
        "- The outputs of the cells will be displayed below it.\n",
        "- You can use the `Table of contents` tab ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB0AAAAbCAIAAAAPqBNFAAABAUlEQVR4AeySQQqCUBiEo70g5GHyAFHrqF21kI4VLqpd0broAHYYA8ED1Mdz9U/xwCfufAwywz+O8r8Zf/o541E/Z8ht9jrs4f8eilexXK8ApHGEPc1+q6o65HntDgRZvsvNbjtbzD3AgE0+b3Jl1kWa3DiO91kWuQNBJpPkfDw97w8PMGCTnzC5zNJpertcAQQZDM0NDpIXNZcaUAYAEWsraXIpADVwdaghSC6a6/aUgREGbPJVkyuzLtLkUgBq4OoQQZBcNNftKQMjDNjkJ0wuM2pAGQAEGQzNDQ6SFzWXGlAGABFrK2lyKQA1GPrABqkBZQAQZDDMfoNTfl/sK/cLAAD//yOd0ZMAAAAGSURBVAMAXYku71pTatsAAAAASUVORK5CYII=) on the left sidebar to jump between sections easily.\n",
        "- Variables that can be modified are marked with `# CHANGE ME`. Read the code comments to understand how and why you might want to adjust that variable.\n",
        "- Keep in mind that **cells are interdependent**. In most cases:\n",
        "  - You must run all previous cells at least once before running the next one.\n",
        "  - Skipping earlier cells may cause later cells to fail or behave unexpectedly.\n",
        "- If you change the contents of a cell, **you need to re-run that cell** for the update to take effect.\n",
        "- **After updating a variable in one cell, you must re-run any other cells that depend on that variable** for the changes to be reflected in their output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06548040",
      "metadata": {
        "id": "06548040"
      },
      "source": [
        "### This cell/section imports the requirements file from Github, downloading the libraries necessary to run our code\n",
        "P.S. yes, this notebook was not authored by Google, but by us at the [University of California](https://c2.ucdavis.edu)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "51a6ab3c",
      "metadata": {
        "id": "51a6ab3c"
      },
      "outputs": [],
      "source": [
        "# install dependencies, this may take a while\n",
        "!pip install -q -r https://raw.githubusercontent.com/AI-Agents-Prompts-to-Multi-Agent-Sys/Democratic-Dialogue/refs/heads/main/requirements.txt 2>/dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72ee2145",
      "metadata": {
        "id": "72ee2145"
      },
      "source": [
        "### Choose Model\n",
        "\n",
        "This cell/section defines the Google Gemini generative AI model you'll be using (there are more to choose from, but not all of them are in the free tier)\n",
        "\n",
        ">***model_name** is the variable to define which gemini model you are using. The free tier choice includes gemini-2.0-flash, gemini-2.0-flash-lite, gemma-3n-e4b-it, gemma-3-27b-it.  \n",
        "They each have different numbers of billion parameters. You can use this variable to test out the ability of different models.*\n",
        "\n",
        ">Keep in mind that each evaluation cycle costs 6 requests. So a model with more than 6 requests per minute is certainly more practical. The more complex the model (generally speaking, the more parameters it has), the less requests you get per minute.  \n",
        "Here are some current rate limits for free tier models from Google: https://ai.google.dev/gemini-api/docs/rate-limits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7ead3d53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ead3d53",
        "outputId": "41ebf482-f662-4f2b-bd08-68e90745cfaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model chosen: gemini-2.0-flash\n"
          ]
        }
      ],
      "source": [
        "model_name = \"gemini-2.0-flash\"  # Recommended Model choice: gemini-2.0-flash, gemini-2.0-flash-lite, gemini-2.5-flash-lite\n",
        "print(f\"Model chosen: {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c97157",
      "metadata": {
        "id": "85c97157"
      },
      "source": [
        "## Customizable variable\n",
        "The following cell/section defines the topic (default is next year's tech trend) and the characters of 6 debaters (you can later change this).\n",
        "\n",
        "***question*** is the main topic that all the conversation will be about.\n",
        "***role_prompts*** is the variable that contains the definition of roles. If you wish to change them also be aware of the next variable.\n",
        "\n",
        "\n",
        "Alternatively, later on, you can also ask the LLM to rewrite the characteristics of the debaters, based on their given names and the specified topic (our default debaters are chosen to debate tech trends). For this, you can enable\n",
        "***rewrite_prompt*** will rewrite the characteristics of the debaters with ***role_prompts*** if set to ***True***.\n",
        "The default is set to False to go with the written descriptions of the debtors seen below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d60a2813",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d60a2813",
        "outputId": "cf8d746c-da71-407b-f9e9-de03025794bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompts will NOT be rewritten by generative LLM!\n"
          ]
        }
      ],
      "source": [
        "# All customizable variables:\n",
        "\n",
        "question = \"What will be the most popular technology trend in next year?\"\n",
        "\n",
        "# 6 roles of evaluation(basic):\n",
        "role_prompts = {\n",
        "    \"Optimist\": \"You are the Optimist Evaluator. Answer the question from a positive, opportunity-focused perspective, highlighting potential advantages and benefits.\",\n",
        "    \"Pessimist\": \"You are the Pessimist Evaluator. Answer the question by focusing on possible downsides, risks, and negative consequences.\",\n",
        "    \"Conservative\": \"You are the Conservative Evaluator. Answer the question from a tradition- and stability-focused perspective, emphasizing continuity and proven approaches.\",\n",
        "    \"Progressive\": \"You are the Progressive Evaluator. Answer the question from an innovation- and change-focused perspective, emphasizing transformative and forward-thinking ideas.\",\n",
        "    \"Authoritarian\": \"You are the Authoritarian Evaluator. Answer the question from a control- and order-focused perspective, emphasizing regulation and compliance.\",\n",
        "    \"Collectivist\": \"You are the Collectivist Evaluator. Answer the question from a communal and shared-benefit perspective, emphasizing fairness, accessibility, and collective advantage.\"\n",
        "}\n",
        "rewrite_prompt = False  # Auto rewrite prompt based on the question and the debater's name if True.\n",
        "\n",
        "if rewrite_prompt:\n",
        "    print(\"Prompts will be rewritten by generative LLM!\")\n",
        "else:\n",
        "    print(\"Prompts will NOT be rewritten by generative LLM!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "813df169",
      "metadata": {
        "id": "813df169"
      },
      "source": [
        "### Running this cell/section sets up the very basic workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "141b597b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "141b597b",
        "outputId": "21cbe219-5e37-4dec-f877-0901ae9692a1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret GEMINI_API_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2485308720.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GEMINI_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GEMINI_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret GEMINI_API_KEY does not exist."
          ]
        }
      ],
      "source": [
        "from typing import TypedDict, Literal, Annotated\n",
        "from operator import add, or_  # reducers for parallel writes\n",
        "from langgraph.graph import StateGraph, END\n",
        "import google.generativeai as genai\n",
        "import copy\n",
        "\n",
        "import sys\n",
        "sys.setrecursionlimit(100)   #change the default limit to 100, incase of deep recursion\n",
        "\n",
        "import os\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "except ImportError:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "print(\"Google Gemini API configured!\")\n",
        "\n",
        "class EvaluationState(TypedDict):\n",
        "    question: str\n",
        "    loop_count: int\n",
        "    evaluations: Annotated[list, add]        # list accumulator\n",
        "    representative_outputs: Annotated[dict, or_]    # representative writes per loop key\n",
        "    continue_: bool\n",
        "    role_prompts_rewrite: dict      # for rewriting base role prompts\n",
        "\n",
        "graph_builder = StateGraph(EvaluationState)\n",
        "print(\"Empty StateGraph created!\")\n",
        "print()\n",
        "\n",
        "# node for rewriting prompts\n",
        "\n",
        "def prompt_engineer_node(state: EvaluationState):\n",
        "    if not rewrite_prompt:\n",
        "        return {}\n",
        "\n",
        "    q = state[\"question\"]\n",
        "\n",
        "    # generate ALL six role prompts in ONE model call\n",
        "    roles = list(role_prompts.keys())\n",
        "    roles_csv = \", \".join(roles)\n",
        "\n",
        "    system_task = (\n",
        "        \"You are a Prompt Engineer for a democratic deliberation system.\\n\"\n",
        "        \"Goal: For EACH role, craft a short, distinct, creative prompt that makes the role answer the QUESTION \"\n",
        "        \"from its worldview. Encourage diversity in tone/angle between roles.\\n\"\n",
        "        \"Constraints:\\n\"\n",
        "        \"- Keep each role prompt concise (< 60 words).\\n\"\n",
        "        \"- DO NOT include formatting/output rules; only describe perspective and what to consider.\\n\"\n",
        "        \"- Return ONLY a JSON object mapping role name -> prompt (no prose, no markdown fences).\\n\"\n",
        "    )\n",
        "\n",
        "    user_payload = (\n",
        "        f\"QUESTION: {q}\\n\\n\"\n",
        "        f\"ROLES: {roles_csv}\\n\\n\"\n",
        "        \"Output JSON with exactly these keys (one per role), values are the rewritten prompts.\"\n",
        "    )\n",
        "\n",
        "    prompt_text = f\"{system_task}\\n{user_payload}\"\n",
        "\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "    try:\n",
        "        resp_text = model.generate_content(prompt_text).text or \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Prompt engineer error: {e}\")\n",
        "        return {}\n",
        "\n",
        "    # JSON extraction\n",
        "    txt = resp_text.strip()\n",
        "    if txt.startswith(\"```\"):\n",
        "        i = txt.find(\"\\n\")\n",
        "        if i != -1:\n",
        "            txt = txt[i+1:]\n",
        "        j = txt.rfind(\"```\")\n",
        "        if j != -1:\n",
        "            txt = txt[:j]\n",
        "        txt = txt.strip()\n",
        "\n",
        "    # Parse JSON\n",
        "    try:\n",
        "        import json\n",
        "        parsed = json.loads(txt)\n",
        "    except Exception:\n",
        "        print(\"Prompt engineer returned non-JSON; keeping existing role_prompts.\")\n",
        "        return {}\n",
        "\n",
        "    new_prompts = role_prompts.copy()\n",
        "    updated_any = False\n",
        "    for r in roles:\n",
        "        v = parsed.get(r)\n",
        "        if isinstance(v, str) and v.strip():\n",
        "            new_prompts[r] = v.strip()\n",
        "            updated_any = True\n",
        "\n",
        "    if updated_any:\n",
        "        print(\"Rewriting role prompts...\")\n",
        "        for r in roles:\n",
        "            print(f\"  - {r}: {new_prompts[r]}\")\n",
        "        print()\n",
        "        return {\"role_prompts_rewrite\": new_prompts}\n",
        "    return {}\n",
        "\n",
        "def dispatch_fn(_state):\n",
        "    return {}\n",
        "\n",
        "from langgraph.constants import START, END\n",
        "graph_builder.add_edge(START, \"PromptEngineer\")\n",
        "graph_builder.add_node(\"PromptEngineer\", prompt_engineer_node)\n",
        "graph_builder.add_edge(\"PromptEngineer\", \"Dispatch\")\n",
        "graph_builder.add_node(\"Dispatch\", dispatch_fn)\n",
        "\n",
        "# Graph Visualization\n",
        "temp = copy.deepcopy(graph_builder)\n",
        "preview = temp.compile()\n",
        "from IPython.display import Image, display\n",
        "print(\"Running this cell/section created a PromptEngineer Node that creates prompts and dispatches the tasks to the different deliberating debaters.\")\n",
        "print(\"Below you can see the very simple StateGraph of the workflow we have so far!\")\n",
        "display(Image(preview.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b5b4ba4"
      },
      "source": [
        "import os\n",
        "\n",
        "if 'GEMINI_API_KEY' in os.environ:\n",
        "    print(\"GEMINI_API_KEY is loaded into environment variables.\")\n",
        "else:\n",
        "    print(\"GEMINI_API_KEY is NOT loaded into environment variables.\")"
      ],
      "id": "3b5b4ba4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "21873258",
      "metadata": {
        "id": "21873258"
      },
      "source": [
        "### Running this cell/section sets up the workflow with the different debaters and the summarizing representative speaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "29d3a3c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "29d3a3c4",
        "outputId": "6df87362-852e-478d-ae48-69cce2080d06"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Node `Representative` already present.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1474812687.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m \u001b[0mgraph_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Representative\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentative_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrole_prompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mgraph_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_gemini_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36madd_node\u001b[0;34m(self, node, action, defer, metadata, input_schema, retry_policy, cache_policy, destinations, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Node `{node}` already present.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEND\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSTART\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Node `{node}` is reserved.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Node `Representative` already present."
          ]
        }
      ],
      "source": [
        "# making a node for each evaluation role\n",
        "\n",
        "def make_gemini_node(role_name: str, prompt: str):\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "\n",
        "    def node_fn(state: EvaluationState):\n",
        "\n",
        "        role_prompt = (\n",
        "            state.get(\"role_prompts_rewrite\", {}).get(role_name)\n",
        "            or prompt\n",
        "        )\n",
        "\n",
        "        chat = model.start_chat(history=[])\n",
        "        prev_loop = str(state[\"loop_count\"] - 1)\n",
        "        # pull prior context (may be empty)\n",
        "        prev_self = \"\"\n",
        "        for e in state.get(\"evaluations\", []):\n",
        "            if e.get(\"loop\") == prev_loop and e.get(\"role\") == role_name:\n",
        "                prev_self = e.get(\"text\", \"\")\n",
        "                break\n",
        "        prev_consensus = state.get(\"representative_outputs\", {}).get(prev_loop, {}).get(\"consensus\", [])\n",
        "        consensus_txt = \"\\n\".join(f\"- {c}\" for c in prev_consensus) if prev_consensus else \"(none)\"\n",
        "\n",
        "        full_prompt = (\n",
        "            f\"{role_prompt}\\n\"\n",
        "            # f\"Question: {state['question']}\\n\"\n",
        "            f\"Consider your prior thoughts and the shared consensus from the last loop if present.\\n\"\n",
        "            f\"Your prior output (may be empty):\\n{prev_self}\\n\"\n",
        "            f\"Shared consensus (3 items, may be empty):\\n{consensus_txt}\\n\"\n",
        "            f\"Now output EXACTLY 3 bullet points, one per line, no numbering, no extra text.\"\n",
        "        )\n",
        "        try:\n",
        "            # print(f\"[{role_name}] Sending prompt: {full_prompt}\")       # debug\n",
        "            response = chat.send_message(full_prompt).text\n",
        "        except Exception as e:\n",
        "            response = f\"[ERROR from {role_name}]: {str(e)}\"\n",
        "        print(f\"[{role_name}]: \\n{response}\")\n",
        "\n",
        "        lc = str(state[\"loop_count\"])\n",
        "        # IMPORTANT: return only the changed key as a partial update\n",
        "        return {\"evaluations\": [{\"loop\": lc, \"role\": role_name, \"text\": response}]}\n",
        "\n",
        "    return node_fn\n",
        "\n",
        "\n",
        "# the representative node\n",
        "\n",
        "def _extract_thought_lines(raw: str) -> list[str]:\n",
        "    # keep only short lines; users/LLMs often provide bullets as lines\n",
        "    lines = [l.strip() for l in raw.split(\"\\n\") if l.strip()]\n",
        "    # de-number bullets like \"1. foo\" -> \"foo\"\n",
        "    cleaned = []\n",
        "    for l in lines:\n",
        "        if l[:2].isdigit() and l[1:2] == '.':\n",
        "            cleaned.append(l[2:].strip())\n",
        "        elif l.startswith(\"-\"):\n",
        "            cleaned.append(l[1:].strip())\n",
        "        else:\n",
        "            cleaned.append(l)\n",
        "    return cleaned[:6]\n",
        "def representative_node(state: EvaluationState):\n",
        "    lc = str(state[\"loop_count\"])\n",
        "    this_loop = [e for e in state.get(\"evaluations\", []) if e.get(\"loop\") == lc]\n",
        "    roles_present = {e.get(\"role\") for e in this_loop}\n",
        "\n",
        "    # Barrier: wait until all 6 roles reported\n",
        "    if len(roles_present) < 6:\n",
        "        return {}  # no-op (return nothing) so downstream doesn't fire\n",
        "\n",
        "    # Idempotency: if already summarized this loop, do nothing\n",
        "    if lc in state.get(\"representative_outputs\", {}):\n",
        "        return {}\n",
        "\n",
        "    # Collect thoughts per role\n",
        "    role_thoughts = {}\n",
        "    for e in this_loop:\n",
        "        role_thoughts.setdefault(e[\"role\"], []).extend(_extract_thought_lines(e.get(\"text\", \"\")))\n",
        "\n",
        "    # Call Gemini to semantically cluster and choose top-3 consensus\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "    thought_dump = \"\\n\".join(\n",
        "        f\"{role}:\\n\" + \"\\n\".join(f\"- {t}\" for t in thoughts)\n",
        "        for role, thoughts in role_thoughts.items()\n",
        "    )\n",
        "\n",
        "    representative_prompt = f\"\"\"\n",
        "You are Representative, a consensus synthesizer. You receive six roles' bullet thoughts.\n",
        "Task:\n",
        "1) Identify semantic clusters across all expressed opinions and find words that represent the most common ideas\n",
        "2) count the votes of support expressed for each topic (maximally 6 votes of support from the six roles)\n",
        "3) based on the vote count, make a ranking of the **three most commonly agreed ideas** and report the number of supporting votes.\n",
        "4) If there has been a previous round, make a summary statement that comments on\n",
        "4a) how the topics evolved (did some merge or change in character?)\n",
        "4b) how the vote count of support changed\n",
        "\n",
        "Six roles' THOUGHTS:\n",
        "{thought_dump}\n",
        "Last round consensus (may be empty for first round):\n",
        "{state.get(\"representative_outputs\", {}).get(str(int(lc)-1), {}).get(\"consensus\", [])}\n",
        "\n",
        "Return ONLY in this format:\n",
        "CONSENSUS:\n",
        "- <idea 1>\n",
        "- <idea 2>\n",
        "- <idea 3>\n",
        "AGREEMENT: <LOW/MEDIUM/HIGH>\n",
        "Summary comment:\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        resp = model.start_chat(history=[]).send_message(representative_prompt).text\n",
        "    except Exception:\n",
        "        resp = (\n",
        "            \"CONSENSUS:\\n- insufficient data\\n- insufficient data\\n- insufficient data\\n\"\n",
        "            \"AGREEMENT: LOW\"\n",
        "        )\n",
        "\n",
        "    # Parse tiny fixed format\n",
        "    consensus, agreement = [], \"LOW\"\n",
        "    try:\n",
        "        lines = [l.strip() for l in resp.splitlines()]\n",
        "        in_cons = False\n",
        "        for l in lines:\n",
        "            if l.upper().startswith(\"CONSENSUS\"):\n",
        "                in_cons = True\n",
        "                continue\n",
        "            if l.upper().startswith(\"AGREEMENT\"):\n",
        "                in_cons = False\n",
        "                parts = l.split(\":\", 1)\n",
        "                if len(parts) == 2:\n",
        "                    agreement = parts[1].strip().upper()\n",
        "                break\n",
        "            if in_cons and l.startswith(\"-\"):\n",
        "                consensus.append(l[1:].strip())\n",
        "        consensus = (consensus + [\"(none)\"]*3)[:3]\n",
        "\n",
        "    except Exception:\n",
        "        consensus, agreement = [\"(parse error)\"]*3, \"LOW\"\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n--- Representative Summary (Loop {state['loop_count']}) ---\")\n",
        "    print(resp)\n",
        "\n",
        "    # IMPORTANT: return only the merged dict for representative_outputs, not the whole state\n",
        "    return {\"representative_outputs\": {lc: {\"consensus\": consensus, \"summary\": f\"Agreement: {agreement}\"}}}\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"Representative\", representative_node)\n",
        "for role, prompt in role_prompts.items():\n",
        "    graph_builder.add_node(role, make_gemini_node(role, prompt))\n",
        "    graph_builder.add_edge(\"Dispatch\", role)\n",
        "    graph_builder.add_edge(role, \"Representative\")\n",
        "\n",
        "# Graph Visualization\n",
        "temp = copy.deepcopy(graph_builder)\n",
        "preview = temp.compile()\n",
        "from IPython.display import Image, display\n",
        "print(\"Running this cell section created the dispatching of debate tasks to the six previously defined debaters and added the representative speaker \\n\"\n",
        "\"who summarizes the different opinions expressed by the debaters. Below you can see the very simple StateGraph of the workflow we have so far!\")\n",
        "display(Image(preview.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9903000",
      "metadata": {
        "id": "f9903000"
      },
      "source": [
        "### Running this cell/section adds your human agency to decide when to stop the democratic deliberation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e728a7",
      "metadata": {
        "id": "80e728a7"
      },
      "outputs": [],
      "source": [
        "# human in the loop node\n",
        "\n",
        "def human_node(state: EvaluationState):\n",
        "    lc = state[\"loop_count\"]\n",
        "\n",
        "    print(\"\\nDo you want to continue to the next evaluation Cycle?\")\n",
        "    decision = input(\"Answer (y/n) to continue: \").lower().strip()\n",
        "\n",
        "    if decision == \"y\":\n",
        "        print(\"\\n\\nContinuing to next round...\\n\\n\")\n",
        "        lc += 1\n",
        "        print(f\"===== Loop {lc} =====\")\n",
        "    # IMPORTANT: only return the changed key\n",
        "    return {\"continue_\": decision == \"y\", \"loop_count\": lc}\n",
        "\n",
        "\n",
        "def check_continue(state: EvaluationState) -> Literal[\"yes\", \"no\"]:\n",
        "    return \"yes\" if state.get(\"continue_\", False) else \"no\"\n",
        "\n",
        "graph_builder.add_edge(\"Representative\", \"Human-Evaluator-in-the-Loop\")\n",
        "graph_builder.add_node(\"Human-Evaluator-in-the-Loop\", human_node)\n",
        "graph_builder.add_conditional_edges(\"Human-Evaluator-in-the-Loop\", check_continue, {\"yes\": \"Dispatch\", \"no\": END})\n",
        "\n",
        "# Graph Visualization\n",
        "temp = copy.deepcopy(graph_builder)\n",
        "preview = temp.compile()\n",
        "from IPython.display import Image, display\n",
        "print(\"Running this cell section created added YOU and your agency!\")\n",
        "print(\"After each round of deliberation, you have the chance to enter the loop and decide if you'd like to continue with another round \\n\"\n",
        "\"(see dotted arrow) or end the democratic exchange.\")\n",
        "print(\"Below you can see the very simple StateGraph of the workflow we have so far!\")\n",
        "display(Image(preview.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c51d2e",
      "metadata": {
        "id": "38c51d2e"
      },
      "source": [
        "### Running this cell/section will start one round of deliberation, including the expressions of the six debaters and the summary top-3 ranking of the representative speaker (scroll to the left at the bottom). To end here, answer the final question on 'continuation' with \"n\" (the cell will run until you do this). If you'd like to go into another round of deliberation, please enter \"y\".\n",
        "\n",
        "If something fails, try re-run the cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b936e2b",
      "metadata": {
        "id": "8b936e2b"
      },
      "outputs": [],
      "source": [
        "# run the workflow and the graph\n",
        "\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "initial_state = {\n",
        "    \"question\": question,\n",
        "    \"loop_count\": 1,\n",
        "    \"evaluations\": [],          # list aggregator\n",
        "    \"representative_outputs\": {},      # dict merger\n",
        "    \"continue_\": True,\n",
        "    \"role_prompts_rewrite\": {}\n",
        "}\n",
        "\n",
        "state = initial_state\n",
        "while state[\"continue_\"]:\n",
        "    print(f\"\\n===== Loop {state['loop_count']} =====\")\n",
        "    state = graph.invoke(state)\n",
        "    # state[\"loop_count\"] += 1      # this is moved to the human interrupt node"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "aiagent",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}